{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Step-by-Step Exploratory Data Analysis (EDA)\n","## Step 1: Load the dataset\n","\n","This gives an overview of the dataset's structure: number of entries, data types, etc."],"metadata":{"id":"EmjvJZhtPLao"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"aPk0WVJBApi-"},"outputs":[],"source":["from sklearn.datasets import fetch_20newsgroups\n","import pandas as pd\n","\n","# Load the 20 Newsgroups dataset\n","newsgroups = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))\n","\n","# Convert to a DataFrame\n","df = pd.DataFrame({'text': newsgroups.data, 'target': newsgroups.target, 'target_names': [newsgroups.target_names[i] for i in newsgroups.target]})\n","\n","# Display basic info\n","df.info()"]},{"cell_type":"code","source":["print(df.head())"],"metadata":{"id":"e7SrBDegYvKd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tabulate import tabulate\n","\n","print(tabulate(df.tail(), headers='keys', tablefmt='psql'))"],"metadata":{"collapsed":true,"id":"phttEXH7Yz8X"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Step 2: Check for missing or null values\n","\n","You might find some missing text data. If there are missing values, decide whether to fill or drop them depending on the extent of the missing data."],"metadata":{"id":"Tzi2nxBLPYv4"}},{"cell_type":"code","source":["# Check for missing values\n","df.isnull().sum()"],"metadata":{"id":"aIxyK8HIZhFH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Step 3: Summary of text lengths\n","\n","This provides insights into the distribution of text lengths. You may use visualizations like histograms to better understand the variation in document lengths."],"metadata":{"id":"TZARt-oxPwFA"}},{"cell_type":"code","source":["# Add a column for text length\n","df['text_length'] = df['text'].apply(len)\n","\n","# Summary statistics of text length\n","df['text_length'].describe()"],"metadata":{"id":"0dfGxiwrZneq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Step 4: Visualize text length distribution\n","\n","This histogram gives you an idea of the typical document length and if there are any extremely short or long documents."],"metadata":{"id":"182bCUOCP8XM"}},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# Plot the distribution of text lengths\n","plt.figure(figsize=(10,6))\n","sns.histplot(df['text_length'], bins=50, kde=True)\n","plt.title('Distribution of Document Lengths')\n","plt.xlabel('Text Length')\n","plt.ylabel('Frequency')\n","plt.show()"],"metadata":{"id":"SWHaQL0tZ0WK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Step 5: Analyze the distribution of topics (categories)\n","This count plot helps visualize the balance (or imbalance) between the different categories/topics in the dataset."],"metadata":{"id":"jTsAeB8vQLuZ"}},{"cell_type":"code","source":["# Plot the distribution of topics\n","plt.figure(figsize=(12,6))\n","sns.countplot(df['target_names'])\n","plt.xticks(rotation=90)\n","plt.title('Distribution of Topics in 20 Newsgroups')\n","plt.xlabel('Topic')\n","plt.ylabel('Number of Documents')\n","plt.show()"],"metadata":{"id":"oCkAMonebBg4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Step 6: Most frequent words (before preprocessing)\n","\n","This returns the 20 most common words across all documents. This gives you a sense of which terms are frequently used before any text cleaning is applied."],"metadata":{"id":"sF4bTh0nQXuF"}},{"cell_type":"code","source":["from collections import Counter\n","import re\n","\n","# Define a function to get the most frequent words in the dataset\n","def get_most_common_words(texts, num_words=20):\n","    all_words = ' '.join(texts).lower()\n","    all_words = re.sub(r'[^a-zA-Z\\s]', '', all_words)  # Remove punctuation and non-alphabet characters\n","    word_list = all_words.split()\n","    word_freq = Counter(word_list)\n","    return word_freq.most_common(num_words)\n","\n","# Get the most frequent words\n","common_words = get_most_common_words(df['text'], num_words=20)\n","common_words"],"metadata":{"id":"AjY1AK-wbKU2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Step 7: Word Cloud Visualization"],"metadata":{"id":"CILRQAD1Qu-_"}},{"cell_type":"code","source":["from wordcloud import WordCloud\n","\n","# Create a word cloud of the most frequent words\n","wordcloud = WordCloud(width=800, height=400, background_color='white').generate(' '.join(df['text']))\n","\n","# Display the word cloud\n","plt.figure(figsize=(10,6))\n","plt.imshow(wordcloud, interpolation='bilinear')\n","plt.axis('off')\n","plt.title('Word Cloud of the 20 Newsgroups Dataset')\n","plt.show()"],"metadata":{"id":"Ck_4MvpVbNmr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Preprocess Text"],"metadata":{"id":"96s8ZUe0RAPd"}},{"cell_type":"code","source":["import re\n","from nltk.corpus import stopwords\n","import nltk\n","nltk.download('stopwords')\n","\n","stop_words = stopwords.words('english')\n","\n","# Define text preprocessing function\n","def preprocess_text(text):\n","    # Remove non-alphabetic characters\n","    text = re.sub(r'[^a-zA-Z]', ' ', text)\n","    # Convert to lowercase\n","    text = text.lower()\n","    # Remove stopwords\n","    text = ' '.join([word for word in text.split() if word not in stop_words])\n","    return text\n","\n","# Apply the preprocessing function to the dataset\n","df['cleaned_text'] = df['text'].apply(preprocess_text)\n","\n","# Display the first few cleaned texts\n","df[['text', 'cleaned_text']].head()\n"],"metadata":{"id":"_xfZWwx9b0CQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Get the most common words after preprocessing\n","common_words_cleaned = get_most_common_words(df['cleaned_text'], num_words=20)\n","common_words_cleaned"],"metadata":{"id":"VchXaQevb9Me"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from wordcloud import WordCloud\n","\n","# Create a word cloud of the most frequent words after preprocessing\n","wordcloud_cleaned = WordCloud(width=800, height=400, background_color='white').generate(' '.join(df['cleaned_text']))\n","\n","# Display the word cloud\n","plt.figure(figsize=(10,6))\n","plt.imshow(wordcloud_cleaned, interpolation='bilinear')\n","plt.axis('off')\n","plt.title('Word Cloud of Cleaned 20 Newsgroups Dataset')\n","plt.show()\n"],"metadata":{"id":"Qlq53_FOcDWS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Preprocess Text (2.0) - This time we will remove tokens that are 1 or 2 characters long."],"metadata":{"id":"kla3LClMRbVg"}},{"cell_type":"code","source":["# Define updated text preprocessing function\n","def preprocess_text(text):\n","    # Remove non-alphabetic characters\n","    text = re.sub(r'[^a-zA-Z]', ' ', text)\n","    # Convert to lowercase\n","    text = text.lower()\n","    # Remove stopwords and words that are 1-2 characters long\n","    text = ' '.join([word for word in text.split() if word not in stop_words and len(word) > 2])\n","    return text\n","\n","# Apply the updated preprocessing function to the dataset\n","df['cleaned_text'] = df['text'].apply(preprocess_text)\n","\n","# Display the first few cleaned texts\n","df[['text', 'cleaned_text']].head()"],"metadata":{"id":"BOqw3l7Ucflo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Get the most common words after preprocessing\n","common_words_cleaned = get_most_common_words(df['cleaned_text'], num_words=20)\n","common_words_cleaned"],"metadata":{"id":"dwI_I1xsckov"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create a word cloud of the most frequent words after preprocessing\n","wordcloud_cleaned = WordCloud(width=800, height=400, background_color='white').generate(' '.join(df['cleaned_text']))\n","\n","# Display the word cloud\n","plt.figure(figsize=(10,6))\n","plt.imshow(wordcloud_cleaned, interpolation='bilinear')\n","plt.axis('off')\n","plt.title('Word Cloud of Cleaned 20 Newsgroups Dataset')\n","plt.show()\n"],"metadata":{"id":"v8-adkokcuxS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# In-class Exercise #1 (15 mins)\n","\n","## 1. Define a function that get the most common words by ``category``\n","## 2. Define a function that generates a word cloud by ``category``"],"metadata":{"id":"PtlRXrCuR2YV"}},{"cell_type":"code","source":["# Define a function to get most common words by category\n","def get_common_words_by_category(category_name):\n","    category_text = df[df['target_names'] == category_name]['cleaned_text']\n","    return get_most_common_words(category_text, num_words=10)\n","\n","# Example: Most common words in 'sci.space' category\n","get_common_words_by_category('sci.space')"],"metadata":{"id":"Ti3Ir34rc8Cj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Function to generate a word cloud for a specific category\n","def generate_wordcloud_by_category(category_name):\n","    # Filter the dataset by the selected category\n","    category_text = df[df['target_names'] == category_name]['cleaned_text'].str.cat(sep=' ')\n","\n","    # Generate a word cloud for the filtered text\n","    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(category_text)\n","\n","    # Display the word cloud\n","    plt.figure(figsize=(10,6))\n","    plt.imshow(wordcloud, interpolation='bilinear')\n","    plt.axis('off')\n","    plt.title(f'Word Cloud for Category: {category_name}')\n","    plt.show()\n","\n","# Example: Generate word cloud for 'sci.space' category\n","generate_wordcloud_by_category('sci.space')"],"metadata":{"id":"iufXZsJgdI-t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Word cloud for 'comp.graphics' category\n","generate_wordcloud_by_category('comp.graphics')\n"],"metadata":{"id":"gQXSIQ32dM8V"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# Word cloud for 'talk.politics.misc' category\n","generate_wordcloud_by_category('talk.politics.misc')"],"metadata":{"id":"pSsjYJGsdSZS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Step 8: Analyze text by category\n","This gives insights into which categories have longer or shorter documents on average."],"metadata":{"id":"sp_qr3rXSfrI"}},{"cell_type":"code","source":["# Calculate the average text length by category\n","avg_length_by_category = df.groupby('target_names')['text_length'].mean().sort_values(ascending=False)\n","\n","# Plot the average text length by category\n","plt.figure(figsize=(12,6))\n","avg_length_by_category.plot(kind='bar')\n","plt.title('Average Text Length by Category')\n","plt.ylabel('Average Text Length')\n","plt.show()"],"metadata":{"id":"FneClOmfSpSs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Step 9: Top Words by Category\n","Let's take a look at the most frequent words in each category after preprocessing."],"metadata":{"id":"y633tkM1TM13"}},{"cell_type":"code","source":["from collections import Counter\n","\n","# Function to get the top words by category\n","def get_top_words_by_category(category_name, num_words=10):\n","    category_text = df[df['target_names'] == category_name]['cleaned_text']\n","    all_words = ' '.join(category_text).split()\n","    word_freq = Counter(all_words)\n","    return word_freq.most_common(num_words)\n","\n","# Example: Top words in 'sci.space' category\n","get_top_words_by_category('sci.space', num_words=10)"],"metadata":{"id":"qB8d-fecTS-9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Step 10: Document Similarity (Cosine Similarity)\n","\n","We can analyze how similar documents are to each other by computing cosine similarity between the documents. This gives us insights into whether documents within a category tend to be more similar.\n","\n","This heatmap shows the similarity between the first 100 documents in the dataset. It can help identify whether some documents are closely related to others."],"metadata":{"id":"SHO93-mDTDTg"}},{"cell_type":"code","source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.metrics.pairwise import cosine_similarity\n","\n","# Vectorize the cleaned text using TF-IDF\n","tfidf_vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')\n","tfidf_matrix = tfidf_vectorizer.fit_transform(df['cleaned_text'])\n","\n","# Compute cosine similarity between documents\n","cosine_sim = cosine_similarity(tfidf_matrix)\n","\n","# Visualize the cosine similarity matrix for the first 100 documents\n","plt.figure(figsize=(10,8))\n","sns.heatmap(cosine_sim[:100, :100], cmap='coolwarm')\n","plt.title('Cosine Similarity Between Documents (First 100)')\n","plt.show()\n"],"metadata":{"id":"NcjoKj2wIcRM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Step 10.1: Text Similarity between Categories\n","\n","The above heatmap is not very informative. Let's compare specific categories\n","\n","To measure the **cosine similarity** between two specific categories, we need to follow these steps:\n","- Filter the dataset for the two categories of interest.\n","- Vectorize the documents using TF-IDF.\n","- Compute cosine similarity between the averaged TF-IDF vectors for the two categories."],"metadata":{"id":"fhHWyde2T64V"}},{"cell_type":"markdown","source":["### Step 1: Filter the Data by Category\n","We will filter the dataset to extract the documents for two categories (e.g., ``sci.space`` and ``comp.graphics``)."],"metadata":{"id":"Cxo0V95BVIJs"}},{"cell_type":"code","source":["# Filter documents for 'sci.space' and 'comp.graphics' categories\n","category_1 = df[df['target_names'] == 'sci.space']['cleaned_text']\n","category_2 = df[df['target_names'] == 'comp.graphics']['cleaned_text']"],"metadata":{"id":"bic4Ja-bIp_v"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Step 2: Vectorize the Documents Using TF-IDF\n","We need to vectorize the documents from each category using the same TF-IDF vectorizer to make their features comparable."],"metadata":{"id":"0QRzYBJhVUgH"}},{"cell_type":"code","source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","# Create a TF-IDF vectorizer\n","tfidf_vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')\n","\n","# Combine the text from both categories for fitting the vectorizer\n","combined_text = pd.concat([category_1, category_2])\n","\n","# Fit and transform the text\n","tfidf_matrix = tfidf_vectorizer.fit_transform(combined_text)\n","\n","# Separate the transformed matrices for the two categories\n","tfidf_category_1 = tfidf_matrix[:len(category_1)]\n","tfidf_category_2 = tfidf_matrix[len(category_1):]\n"],"metadata":{"id":"vxtuTzCGI4KS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Step 3: Compute Average TF-IDF Vectors for Each Category\n","To compare categories, we can calculate the **average TF-IDF vector** for each category and then compute the cosine similarity between them."],"metadata":{"id":"CmyDzKpRVeuR"}},{"cell_type":"code","source":["import numpy as np\n","\n","# Compute the mean TF-IDF vector for each category\n","mean_tfidf_category_1 = np.mean(tfidf_category_1.toarray(), axis=0)\n","mean_tfidf_category_2 = np.mean(tfidf_category_2.toarray(), axis=0)"],"metadata":{"id":"IaLTP3zNI9Hn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Step 4: Compute Cosine Similarity Between the Two Categories\n","Finally, we compute the cosine similarity between the two average TF-IDF vectors.\n","\n","The resulting cosine similarity value (between 0 and 1) gives us a measure of how similar the two categories are in terms of their TF-IDF vector representations. A value closer to 1 indicates that the two categories share more similar word distributions, while a value closer to 0 indicates less similarity."],"metadata":{"id":"0ryPLdOZVoRj"}},{"cell_type":"code","source":["from sklearn.metrics.pairwise import cosine_similarity\n","\n","# Reshape the mean vectors for cosine similarity computation\n","mean_tfidf_category_1 = mean_tfidf_category_1.reshape(1, -1)\n","mean_tfidf_category_2 = mean_tfidf_category_2.reshape(1, -1)\n","\n","# Compute cosine similarity between the two categories\n","cosine_sim_between_categories = cosine_similarity(mean_tfidf_category_1, mean_tfidf_category_2)\n","\n","# Output the cosine similarity\n","cosine_sim_between_categories[0][0]\n"],"metadata":{"id":"40d9a6XgJB7E"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","# Filter documents for 'sci.space' and 'comp.graphics' categories\n","category_1 = df[df['target_names'] == 'sci.space'].head(10)  # Select first 10 docs from sci.space\n","category_2 = df[df['target_names'] == 'comp.graphics'].head(10)  # Select first 10 docs from comp.graphics\n","\n","# Create a TF-IDF vectorizer\n","tfidf_vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')\n","\n","# Combine the text from both categories for fitting the vectorizer\n","combined_text = pd.concat([category_1, category_2])\n","\n","# Fit and transform the text\n","tfidf_matrix = tfidf_vectorizer.fit_transform(combined_text['cleaned_text'])\n","\n","# Separate the transformed matrices for the two categories\n","tfidf_category_1 = tfidf_matrix[:len(category_1)]\n","tfidf_category_2 = tfidf_matrix[len(category_1):]\n","\n","# Compute pairwise cosine similarity between the two categories\n","cosine_sim_matrix = cosine_similarity(tfidf_category_1, tfidf_category_2)\n","\n","# Visualize the cosine similarity matrix using a heatmap\n","plt.figure(figsize=(10, 8))\n","sns.heatmap(cosine_sim_matrix, annot=True, cmap='coolwarm', xticklabels=False, yticklabels=False)\n","plt.title(\"Cosine Similarity between Documents from 'sci.space' and 'comp.graphics'\")\n","plt.xlabel(\"Category 2: comp.graphics\")\n","plt.ylabel(\"Category 1: sci.space\")\n","plt.show()\n"],"metadata":{"id":"4Ghtup6IJs-X"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Topic Modeling\n","\n","## Step 1: Install Required Libraries"],"metadata":{"id":"1-ILaJeQ-d1j"}},{"cell_type":"code","source":["pip install numpy pandas scikit-learn nltk pyLDAvis"],"metadata":{"id":"pbg3gowT-mIA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Step 2: Load the Dataset\n","\n","We'll use the ``fetch_20newsgroups`` function from ``scikit-learn`` to load the dataset."],"metadata":{"id":"BDPTXMEz-qlz"}},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.datasets import fetch_20newsgroups\n","\n","# Load the dataset\n","newsgroups = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))\n","documents = newsgroups.data\n"],"metadata":{"id":"TUCcIEnm-w-D"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Step 3: Text Preprocessing\n","\n","We need to preprocess the text by tokenizing, removing stop words, and stemming/lemmatizing the words."],"metadata":{"id":"Ba9LRmZX-0X_"}},{"cell_type":"code","source":["import nltk\n","from nltk.corpus import stopwords\n","from nltk.stem import PorterStemmer\n","import re\n","\n","# Download NLTK resources\n","nltk.download('stopwords')\n","stop_words = set(stopwords.words('english'))\n","\n","def preprocess_text(text):\n","    # Remove special characters and digits\n","    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n","    # Tokenize the text\n","    tokens = text.lower().split()\n","    # Remove stop words and stem the words\n","    ps = PorterStemmer()\n","    tokens = [ps.stem(word) for word in tokens if word not in stop_words]\n","    return ' '.join(tokens)\n","\n","# Preprocess the documents\n","preprocessed_documents = [preprocess_text(doc) for doc in documents]\n"],"metadata":{"id":"VxDv8jVG-_D1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Step 4: Vectorization\n","Convert the preprocessed text into a numerical format using the TF-IDF vectorizer."],"metadata":{"id":"yl3jt3SM_TVQ"}},{"cell_type":"code","source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","# Create a TF-IDF Vectorizer\n","vectorizer = TfidfVectorizer(max_features=1000)  # Limit to top 1000 features\n","X = vectorizer.fit_transform(preprocessed_documents)"],"metadata":{"id":"cxKOphOa_VyZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Step 5: Topic Modeling with LDA\n","We'll use LDA for topic modeling on the TF-IDF matrix."],"metadata":{"id":"DTbk7A_vAHnS"}},{"cell_type":"code","source":["from sklearn.decomposition import LatentDirichletAllocation\n","\n","# Set the number of topics\n","num_topics = 10\n","lda = LatentDirichletAllocation(n_components=num_topics, random_state=42)\n","lda.fit(X)\n"],"metadata":{"id":"3ZD4ZIkvAMnX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Step 6: Display Topics\n","Now let's display the topics along with the top words associated with each topic."],"metadata":{"id":"CylJfYoMAR5o"}},{"cell_type":"code","source":["def display_topics(model, feature_names, no_top_words):\n","    for topic_idx, topic in enumerate(model.components_):\n","        print(f\"Topic {topic_idx + 1}:\")\n","        print(\" \".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]))\n","\n","# Get feature names and display topics\n","feature_names = vectorizer.get_feature_names_out()\n","no_top_words = 10\n","display_topics(lda, feature_names, no_top_words)\n"],"metadata":{"id":"wY-y3Ya7AVKK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Step 7: Visualizing the Topics\n","\n","To visualize the topics, we can use ``pyLDAvis``."],"metadata":{"id":"Tv_3_6dMAXmS"}},{"cell_type":"code","source":["import pyLDAvis\n","import pyLDAvis.lda_model\n","\n","# Prepare LDA visualization\n","pyLDAvis.enable_notebook()\n","vis = pyLDAvis.lda_model.prepare(lda, X, vectorizer, mds='tsne')\n","pyLDAvis.display(vis)\n"],"metadata":{"id":"JHESb-NAAhhw"},"execution_count":null,"outputs":[]}]}